{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import GPy\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import math\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from pandas.tools.plotting import autocorrelation_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class data(object):\n",
    "    #first functional and 1-d structures\n",
    "    raw_data = None\n",
    "    struct_data = None\n",
    "    all_train = None\n",
    "    all_test = None\n",
    "    \n",
    "    #will keep the augmented stuff separate this is functional aug\n",
    "    aug_raw_data = None\n",
    "    aug_struct_data = None\n",
    "    all_aug_train = None\n",
    "    all_aug_test = None\n",
    "    samp_aug_train = None\n",
    "    samp_aug_test = None\n",
    "    \n",
    "    #for the ou seasonality\n",
    "    seasonal = None\n",
    "    \n",
    "    def __init__(self, seed, name, years, days, ob_year, ob_day, reset, add_seas, laplace, sigma):\n",
    "        self.seed = seed\n",
    "        self.name = name\n",
    "        self.years = years\n",
    "        self.days = days\n",
    "        self.ob_year = ob_year\n",
    "        self.ob_day = ob_day\n",
    "        self.reset = reset\n",
    "        \n",
    "        #for OU process\n",
    "        self.mu= 0.0 #still looking at ou cal v GP\n",
    "        self.theta=0.008\n",
    "        self.sigma= sigma #1.2#0.1#0.01 #0.4\n",
    "        \n",
    "        #fix for the simulations\n",
    "        self.sim_adjustment_val = 0\n",
    "        self.add_seas = add_seas\n",
    "        self.laplace = laplace\n",
    "        self.get_raw_data()\n",
    "    \n",
    "    def get_raw_data(self):\n",
    "        n = list(range(1,self.years+1))\n",
    "        ob = list(range(1,self.days+1))\n",
    "        c = list(itertools.product(n, ob))\n",
    "        \n",
    "        flat_combo = [[elem[0],elem[1]] for elem in c]\n",
    "        self.raw_data = pd.DataFrame(flat_combo, columns=['Year', 'Day'])   \n",
    "        self.raw_data['Ones']=1\n",
    "        self.raw_data['Day_Index'] = self.raw_data['Ones'].cumsum()\n",
    "        self.raw_data.drop('Ones', axis=1, inplace=True)  \n",
    "    \n",
    "        self.raw_data['TPrices'] = self.OU(self.mu, self.theta, self.sigma)\n",
    "        self.raw_data['NoiselessPrice'] = self.raw_data['TPrices']\n",
    "        self.raw_data['OPrice']= self.raw_data['TPrices'].shift(1)\n",
    "        self.raw_data['OPrice'][0]= self.raw_data['OPrice'][1]\n",
    "        \n",
    "    \n",
    "    def get_aug_raw_data(self):\n",
    "        n=list(range(1,self.years+1))\n",
    "        ob=list(range(1,self.days+1))\n",
    "        c = list(itertools.product(n, ob))\n",
    "        tar=list(range(1,self.days+1))\n",
    "        combo = list(itertools.product(c,tar))\n",
    "        flat_combo = [[elem[0][0],elem[0][1],elem[1]] for elem in combo]\n",
    "\n",
    "        self.aug_raw_data = pd.DataFrame(flat_combo, columns=['Year', 'Ob_day', 'Tar_day',])\n",
    "        self.aug_raw_data['Delta']= self.aug_raw_data['Tar_day'] - self.aug_raw_data['Ob_day']\n",
    "\n",
    "        prices = self.OU(self.mu, self.theta, self.sigma)\n",
    "        \n",
    "        pd_func = pd.DataFrame(prices)\n",
    "        repeat_stock_num = int(self.aug_raw_data.shape[0]/ pd_func.shape[0])\n",
    "        \n",
    "        \n",
    "        #needs tidying but here I am creating a signal feature for augmented\n",
    "        df = pd_func.copy()\n",
    "        df['OPrice'] = pd_func.shift(1)\n",
    "\n",
    "        df['Up'] = df['OPrice']>df['OPrice'].shift(1)\n",
    "        df['Plus'] = (df['OPrice']<3.2)&(df['Up']==True)\n",
    "        df['Minus'] = (df['OPrice']>-3.2)&(df['Up']==False)\n",
    "        df['Flat'] = (df['Minus']==False) &(df['Plus']==False)\n",
    "        df['S1']=0\n",
    "        df['S2']=0\n",
    "        df['S3']=0\n",
    "\n",
    "        df.loc[df['Plus']==True,'S1']=1\n",
    "        df.loc[df['Minus']==True,'S2']=1\n",
    "        df.loc[df['Flat']==True,'S3']=1\n",
    "\n",
    "        df.drop(['Up','Plus','Minus','Flat'],axis=1, inplace=True)\n",
    "        #print(df)\n",
    "        \n",
    "        #ob price\n",
    "        lastprice = list(pd_func.shift(1).values)\n",
    "        lastprice[0]=lastprice[1] #for now just copy the first price\n",
    "        lastprice = np.array(lastprice*repeat_stock_num).reshape(self.days,-1).T\n",
    "        self.aug_raw_data['OPrice'] = lastprice.reshape(-1)\n",
    "        \n",
    "        #Signal\n",
    "        raw_signal = list(df['S1'].values)\n",
    "        signal = np.array(raw_signal*repeat_stock_num).reshape(self.days,-1).T\n",
    "        self.aug_raw_data['S1'] = signal.reshape(-1)\n",
    "        \n",
    "        raw_signal = list(df['S2'].values)\n",
    "        signal = np.array(raw_signal*repeat_stock_num).reshape(self.days,-1).T\n",
    "        self.aug_raw_data['S2'] = signal.reshape(-1)\n",
    "        \n",
    "        raw_signal = list(df['S3'].values)\n",
    "        signal = np.array(raw_signal*repeat_stock_num).reshape(self.days,-1).T\n",
    "        self.aug_raw_data['S3'] = signal.reshape(-1)\n",
    "        \n",
    "        \n",
    "        #trpice\n",
    "        self.aug_raw_data['TPrices'] = np.tile(prices.reshape(self.years,self.days),self.days).reshape((-1,))\n",
    "                                                                                                                     \n",
    "        #ditch values where we look back in time\n",
    "        self.aug_raw_data = self.aug_raw_data[self.aug_raw_data[\"Delta\"] >= 0]\n",
    "        self.aug_raw_data = self.aug_raw_data.reset_index(drop=True)\n",
    "                \n",
    "    def mk_tr_tst(self):\n",
    "        #create test set and train set as well as being able to preprocess\n",
    "        self.struct_data = self.raw_data.copy()\n",
    "\n",
    "        #create copies of yes/ ob day and tar day in order to preprocess some and easily be able to refer to days\n",
    "        self.struct_data['Year_copy']= self.struct_data['Year']\n",
    "        self.struct_data['Day_copy']= self.struct_data['Day']\n",
    "\n",
    "        #how much training data \n",
    "        train_days = self.ob_year*self.days+self.ob_day\n",
    "        all_train_data = self.struct_data[:train_days] \n",
    "\n",
    "        #fit our scaling on all our training data, we will apply the same to test\n",
    "        #std_scale = preprocessing.StandardScaler().fit(all_train_data)\n",
    "        #std_scale_TP = preprocessing.StandardScaler().fit(all_train_data['TPrices'])\n",
    "\n",
    "        #now transform all our data on the train data scaling keep last two columns so i can still index\n",
    "        #all_scaled_data = self.struct_data #std_scale.transform(self.struct_data)\n",
    "        #self.struct_data[self.struct_data.columns[:-2]]=all_scaled_data[:,:-2]\n",
    "\n",
    "        firstprice_bool = (self.struct_data['Day_copy']==1) \n",
    "        firstprices = list(self.struct_data[firstprice_bool]['TPrices'])\n",
    "        \n",
    "        #reset prices to zero each year a la chapados if this option is chosen\n",
    "        if self.reset:\n",
    "            for i in range(1,self.years+1):\n",
    "                row_index =  self.struct_data['Year_copy']==i\n",
    "                self.struct_data.loc[row_index, 'TPrices'] -= firstprices[i-1]\n",
    "\n",
    "        self.all_train = self.struct_data[:train_days]\n",
    "\n",
    "        #just test on everything for now \n",
    "        bool_test = (self.struct_data['Year_copy']>=0) & (self.struct_data['Year_copy']<self.ob_year+2)\n",
    "        self.all_test = self.struct_data[bool_test]  \n",
    "\n",
    "        #for outside experiments I will need to scale the data in the same way\n",
    "        #return std_scale_TP\n",
    "    \n",
    "    def mk_aug_tr_tst(self, max_delta):\n",
    "        #create test set and train set as well as being able to preprocess\n",
    "        self.aug_struct_data = self.aug_raw_data.copy()\n",
    "\n",
    "        #create copies of yes/ ob day and tar day in order to preprocess some and easily be able to refer to days\n",
    "        self.aug_struct_data['Year_copy']= self.aug_struct_data['Year']\n",
    "        self.aug_struct_data['Ob_day_copy']= self.aug_struct_data['Ob_day']\n",
    "        self.aug_struct_data['Tar_day_copy']= self.aug_struct_data['Tar_day']\n",
    "        self.aug_struct_data['Delta_copy']= self.aug_struct_data['Delta'] \n",
    "\n",
    "        #the train and test should overlap over the last data point - take this off when I am sure it is working\n",
    "        tr_data_bool = (~((self.aug_struct_data['Year']>=self.ob_year+1) | (self.aug_struct_data['Delta']>max_delta))\n",
    "                           |((self.aug_struct_data['Year']==self.ob_year+1) & (self.aug_struct_data['Delta']<max_delta) \n",
    "                            & (self.aug_struct_data['Ob_day']+self.aug_struct_data['Delta']<=self.ob_day)))\n",
    "\n",
    "        self.all_aug_train = self.aug_struct_data[tr_data_bool]\n",
    "\n",
    "        #fit our scaling on all our training data, we will apply the same to test\n",
    "        #std_scale = preprocessing.StandardScaler().fit(self.all_aug_train)\n",
    "        #std_scale_TP = preprocessing.StandardScaler().fit(self.all_aug_train['TPrices'])\n",
    "\n",
    "        #transform our training data, but leave the last 4 copy columns which we use for checks and indexing\n",
    "        #scaled_train_data = self.all_aug_train#std_scale.transform(self.all_aug_train)\n",
    "        #self.all_aug_train[self.all_aug_train.columns[:-4]]=scaled_train_data[:,:-4]\n",
    "        \n",
    "        \"\"\"\n",
    "        ###examine scaling and inverse scaling as well as setting each years prices to begin with 0\n",
    "        firstprice_bool = (all_train_data['Ob_day_copy']==1) & (all_train_data['Delta_copy']==0)\n",
    "        all_train_data[all_train_data['Year_copy']==1]\n",
    "\n",
    "        firstprices = list(all_train_data[firstprice_bool]['TPrices'])\n",
    "\n",
    "        all_train_data_test = all_train_data\n",
    "\n",
    "        #10 years\n",
    "        num_years = 10\n",
    "        for i in range(1,num_years+1):\n",
    "            row_index =  all_train_data['Year_copy']==i\n",
    "            all_train_data.loc[row_index, 'TPrices'] -= firstprices[i-1]\n",
    "\n",
    "        all_train_data.head()\n",
    "        \"\"\"\n",
    "        \n",
    "        #do the same for the test data again hard coded right now\n",
    "        tst_data_bool = ((self.aug_struct_data['Year']==self.ob_year+1)  & (self.aug_struct_data['Ob_day']==self.ob_day)\n",
    "                        & (self.aug_struct_data['Delta']>0))\n",
    "        self.all_aug_test= self.aug_struct_data[tst_data_bool]\n",
    "        \n",
    "        #scaled_test_data = self.all_aug_test#std_scale.transform(self.all_aug_test)\n",
    "        #self.all_aug_test[self.all_aug_test.columns[:-4]]=scaled_test_data[:,:-4]\n",
    "        \n",
    "        #return std_scale_TP\n",
    "        \n",
    "    def sample(self,num_samples):\n",
    "    \n",
    "        #add on all delta zeros after\n",
    "        sample_bool = ((self.all_aug_train['Ob_day_copy'])!=(self.all_aug_train['Tar_day_copy']))\n",
    "\n",
    "        train_data = self.all_aug_train[sample_bool].reset_index(drop=True)\n",
    "\n",
    "        #sampling function\n",
    "        rows = np.random.choice(train_data.shape[0], num_samples, replace=False)\n",
    "        rows = np.sort(rows)\n",
    "\n",
    "        sampled_train = train_data.ix[rows]\n",
    "        df_XYtrain = pd.DataFrame(sampled_train) \n",
    "        delta_zero = self.all_aug_train[(self.all_aug_train['Delta_copy']==0)]\n",
    "        #how we sample seems important - go out to max delta...not including all delta zeros\n",
    "        #but from the beginning of that year\n",
    "        #print(delta_zero[delta_zero['Year_copy']==10])\n",
    "        self.samp_aug_train = pd.concat([df_XYtrain, delta_zero[self.ob_year*self.days:]], axis=0)\n",
    "        #self.samp_aug_train = df_XYtrain\n",
    "    \n",
    "        self.samp_aug_test = pd.concat([delta_zero, self.all_aug_test], axis=0)\n",
    "        \n",
    "    def plot_auto(self, filename=None):\n",
    "        autocorrelation_plot(self.struct_data['TPrices'])\n",
    "        if filename:\n",
    "            plt.savefig('./figs/'+filename)\n",
    "            \n",
    "    def plot(self, filename=\"stock\"):\n",
    "        if self.name == \"ou\":\n",
    "            plt.title(\"OU process\")\n",
    "            plt.ylabel(\"Price\")\n",
    "            plt.plot(self.struct_data['TPrices'],'-b')\n",
    "            plt.savefig('./figs/ou.png')\n",
    "    \n",
    "    def plot_curves(self,filename=\"functional\"):\n",
    "        #Now look at the functional version\n",
    "        X_train = self.all_train\n",
    "        for i in range(1,self.years+1):\n",
    "            prices = X_train[X_train['Year_copy']==i]['TPrices']\n",
    "            day  = X_train[X_train['Year_copy']==i]['Day_copy']\n",
    "            ymin, ymax = plt.ylim()\n",
    "            plt.vlines(self.ob_day,ymin,ymax)\n",
    "            plt.plot(day, prices)\n",
    "        #plt.savefig('./figs/'+filename)\n",
    "     \n",
    "    def OU(self, mu, theta, sigma):\n",
    "        #The above discretisation is only valid for very small dt\n",
    "        # Gillespie 1996, See also “Monte Carlo Simulation of Stochastic Processes”\n",
    "    \n",
    "        np.random.seed(self.seed)\n",
    "        t_0 = 1      # define model parameter\n",
    "        t_end = self.years*self.days\n",
    "        length = self.years*self.days\n",
    "\n",
    "        t = np.linspace(t_0,t_end,length)  # define time axis\n",
    "        dt = np.mean(np.diff(t))\n",
    "\n",
    "        y_gill = np.zeros(length)\n",
    "        y_gill[0] = 0 #use a fixed starting point#np.random.normal(loc=0.0,scale=1.0)\n",
    "\n",
    "        drift = lambda y,t: theta*(mu-y)      # define drift term\n",
    "        diffusion = lambda y,t: sigma# define diffusion term\n",
    "        noise = np.random.normal(loc=0.0,scale=1.0,size=length)*np.sqrt(dt) #define noise process\n",
    "        \n",
    "        #laplace = True\n",
    "        #fat tails laplace noise\n",
    "        if laplace:\n",
    "            noise = np.random.laplace(loc=0.0,scale=1.0,size=length)*np.sqrt(dt)\n",
    "\n",
    "        # solve SDE\n",
    "        for i in range(1,length):\n",
    "            y_gill[i] = np.exp(-theta*dt)*y_gill[i-1] + (1- np.exp(-theta*dt))*mu  \\\n",
    "                                              + sigma*noise[i]* np.sqrt((1-np.exp(-2*theta*dt)/2*theta))\n",
    "        #add more structure\n",
    "        #note one subtlety if i am adding ou noise i currently want it unrelated to the function i add to\n",
    "        #when simulating going forward from a certain level we simulate from the level of the function and\n",
    "        #the noise together - so simulate taking off the level of the function at that point and then add\n",
    "        #that noise to the function - unfortunately this hadn't been designed from the start and the value\n",
    "        #we need to take off is hidden in here...lets call it sim_adjustment_val\n",
    "        \n",
    "        #add_seas = True\n",
    "        if add_seas:\n",
    "            \n",
    "            #seasonal = np.tile(np.sin(np.linspace(1,250,250)*2*math.pi/250)*4,10)\n",
    "            seasonal = np.tile(np.sin(np.linspace(1,length/10,length/10)*2*math.pi/(length/10))*4,10)\n",
    "            self.seasonal = seasonal #-np.mean(seasonal)\n",
    "            \n",
    "            #adjustment value so the OU mean reverts according to its vol level, not seasonal too\n",
    "            self.sim_adjustment_val = self.seasonal[self.days*self.ob_year+self.ob_day]\n",
    "            return(y_gill+seasonal)\n",
    "        else:\n",
    "            return(y_gill)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class opt(object):\n",
    "    X=None\n",
    "    y=None\n",
    "    Xpred=None\n",
    "    ytest=None\n",
    "    ll=None\n",
    "    var=None\n",
    "    mod=None\n",
    "    \n",
    "    def __init__(self, train_data, test_data, feat_lst, years, days, obs_year, obs_day):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.feat_lst = feat_lst\n",
    "        self.years = years\n",
    "        self.days = days\n",
    "        self.obs_year = obs_year\n",
    "        self.obs_day = obs_day\n",
    "        self.prep4opt()\n",
    "    \n",
    "    def prep4opt(self):\n",
    "        self.X = (self.train_data[self.feat_lst].as_matrix())\n",
    "        self.y = ((self.train_data[['TPrices']]).as_matrix())\n",
    "\n",
    "        self.Xpred = (self.test_data[self.feat_lst].as_matrix())\n",
    "        self.ytest = ((self.test_data[['TPrices']]).as_matrix())\n",
    "\n",
    "    \n",
    "    def run_opt(self,kern,restarts, aug=False):    \n",
    "        k_input_d = self.X.shape[1]\n",
    "        \n",
    "        if not aug:\n",
    "            if kern == \"rat\":\n",
    "                k = GPy.kern.RatQuad(input_dim=k_input_d, variance=1.0, lengthscale=1.,ARD=True) + GPy.kern.Bias(input_dim=k_input_d)\n",
    "            elif kern == \"mat\":\n",
    "                k = GPy.kern.Matern32(input_dim=k_input_d, variance=1.0, lengthscale=1.,ARD=True) + GPy.kern.Bias(input_dim=k_input_d)\n",
    "            elif kern == \"ou\":\n",
    "                    k = GPy.kern.OU(input_dim=k_input_d, variance=1.0, lengthscale=1.,ARD=True) + GPy.kern.Bias(input_dim=k_input_d)\n",
    "            elif kern == \"rbf\":\n",
    "                k = GPy.kern.RBF(input_dim=k_input_d, variance=1.0, lengthscale=1.,ARD=True) + GPy.kern.Bias(input_dim=k_input_d)\n",
    "            elif kern == \"linear\":\n",
    "                k = GPy.kern.Linear(input_dim=k_input_d,ARD=True)\n",
    "            elif kern ==\"per\":\n",
    "                if k_input_d ==1:\n",
    "                    k=GPy.kern.PeriodicExponential(input_dim=1, variance=1., lengthscale=1.) +\\\n",
    "                    GPy.kern.OU(input_dim=k_input_d, variance=1.0, lengthscale=1.)\n",
    "                else:\n",
    "                    k=GPy.kern.PeriodicExponential(input_dim=1, variance=1., lengthscale=1.,active_dims=[1]) +\\\n",
    "                    GPy.kern.OU(input_dim=k_input_d, variance=1.0, lengthscale=1.,ARD=True)\n",
    "        elif aug:\n",
    "            if kern == \"ou\":\n",
    "                k = GPy.kern.OU(input_dim=k_input_d, variance=1.0, lengthscale=1.,ARD=True) + GPy.kern.Bias(input_dim=k_input_d)    \n",
    "            #if using feature  \n",
    "                if k_input_d ==6:\n",
    "                    k1 = GPy.kern.OU(input_dim=k_input_d-3, variance=1.0, lengthscale=1.,ARD=True, active_dims=[0,1,2]) + GPy.kern.Bias(input_dim=k_input_d) \n",
    "                    k2 = GPy.kern.RBF(input_dim=3, variance=1.0, lengthscale=1.,ARD=True, active_dims=[3,4,5])\n",
    "                    k=k1+k2\n",
    "                elif k_input_d ==7:\n",
    "                    k1 = GPy.kern.OU(input_dim=4, variance=1.0, lengthscale=1.,ARD=True, active_dims=[0,1,2,3]) + GPy.kern.Bias(input_dim=k_input_d) \n",
    "                    k2 = GPy.kern.RBF(input_dim=3, variance=1.0, lengthscale=1.,ARD=True, active_dims=[4,5,6])\n",
    "                    k=k1+k2\n",
    "            elif kern == \"rbf\":\n",
    "                if k_input_d ==6:\n",
    "                    k1 = GPy.kern.RBF(input_dim=k_input_d-3, variance=1.0, lengthscale=1.,ARD=True, active_dims=[0,1,2]) + GPy.kern.Bias(input_dim=k_input_d) \n",
    "                    k2 = GPy.kern.RBF(input_dim=3, variance=1.0, lengthscale=1.,ARD=True, active_dims=[3,4,5])\n",
    "                    k=k1+k2\n",
    "                elif k_input_d ==7:\n",
    "                    k1 = GPy.kern.RBF(input_dim=4, variance=1.0, lengthscale=1.,ARD=True, active_dims=[0,1,2,3]) + GPy.kern.Bias(input_dim=k_input_d) \n",
    "                    k2 = GPy.kern.RBF(input_dim=3, variance=1.0, lengthscale=1.,ARD=True, active_dims=[4,5,6])\n",
    "                    k=k1+k2\n",
    "                k = GPy.kern.Linear(input_dim=k_input_d,ARD=True)\n",
    "            elif kern == \"per\":   \n",
    "                k=GPy.kern.PeriodicExponential(input_dim=1, variance=1., lengthscale=1.,active_dims=[4]) +\\\n",
    "                    GPy.kern.OU(input_dim=k_input_d, variance=1.0, lengthscale=1.,ARD=True)\n",
    "                k = GPy.kern.Matern32(input_dim=k_input_d, variance=1.0, lengthscale=1.,ARD=True) + GPy.kern.Bias(input_dim=k_input_d)\n",
    "            elif kern == \"rat\":\n",
    "                k = GPy.kern.RatQuad(input_dim=k_input_d, variance=1.0, lengthscale=1.,ARD=True) + GPy.kern.Bias(input_dim=k_input_d)    \n",
    "            elif kern == \"mat\":\n",
    "                k = GPy.kern.Matern32(input_dim=k_input_d, variance=1.0, lengthscale=1.,ARD=True) + GPy.kern.Bias(input_dim=k_input_d)\n",
    "        \n",
    "        self.mod = GPy.models.GPRegression(self.X, self.y, k) \n",
    "        \n",
    "        self.mod.constrain_positive('')\n",
    "        self.mod.optimize(messages=True)\n",
    "        self.mod.optimize_restarts(num_restarts=restarts)  \n",
    "        \n",
    "        #self.mod.Gaussian_noise.constrain_fixed(0.0)\n",
    "        self.ll = self.mod.log_likelihood()\n",
    "        #self.ypred, self.var = self.mod.predict_noiseless(self.Xpred)\n",
    "        self.ypred, self.var = self.mod.predict(self.Xpred)\n",
    "        return self.ypred, self.var, self.ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_result1(all_args):\n",
    "    \n",
    "    f, axarr= plt.subplots(2,2,figsize=(20, 10))\n",
    "\n",
    "    for i, arg in enumerate(all_args,1):\n",
    "        title = arg[0]\n",
    "        filename = arg[1]\n",
    "        obs_day = arg[2]\n",
    "        first_day = arg[3]\n",
    "        num_days = arg[4]\n",
    "        xfit = arg[5]\n",
    "        ypred = arg[6]\n",
    "        ytest = arg[7]\n",
    "        var = arg[8]\n",
    "        ll = arg[9]\n",
    "             \n",
    "        if i == 1:\n",
    "            ax = axarr[0,0]\n",
    "        elif i==2:\n",
    "            ax = axarr[0,1]\n",
    "        elif i==3:\n",
    "            ax = axarr[1,0]\n",
    "        elif i==4:\n",
    "            ax = axarr[1,1]\n",
    "\n",
    "        print(\"LL:\"+str(ll))\n",
    "        dypred = 1.96*np.sqrt(var).flatten()\n",
    "        ypred = ypred.flatten()\n",
    "        ytest = ytest.flatten()\n",
    "\n",
    "        ax.plot(xfit[first_day:num_days],ytest[first_day:num_days], 'g-',label='Train Prices')\n",
    "        ax.plot(xfit[obs_day:num_days],ytest[obs_day:num_days],'b-', label='Test Prices')\n",
    "        ax.plot(xfit[first_day:num_days],ypred[first_day:num_days], '-', color='black', label='Prediction')\n",
    "        ax.fill_between(xfit[first_day:obs_day], (ypred[first_day:obs_day]- dypred[first_day:obs_day]), \n",
    "                         (ypred[first_day:obs_day] + dypred[first_day:obs_day]), color='blue', alpha=.2)\n",
    "        ax.fill_between(xfit[obs_day:num_days], (ypred[obs_day:num_days]- dypred[obs_day:num_days]), \n",
    "                         (ypred[obs_day:num_days] + dypred[obs_day:num_days]), color='yellow', alpha=.5, label='Conf Int')\n",
    "\n",
    "        ymin, ymax = ax.get_ylim()\n",
    "        ax.vlines(obs_day,ymin,ymax)\n",
    "        #print(title)\n",
    "        \n",
    "        ax.set_title(title)\n",
    "        ax.legend(loc='best')\n",
    "        ax.grid(which='major')\n",
    "    #plt.savefig('./figs/'+filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ou_cal(data,delta,LS=True):\n",
    "    #Calibrating the OU process - using either maximum likelihood or least squares\n",
    "    #delta =1.0\n",
    "    #test_func = y_gill[:pred_day]\n",
    "    test_func = data\n",
    "    n = test_func.shape[0]-1\n",
    "\n",
    "    Sx  = np.sum(test_func[:-1])\n",
    "    Sy  = np.sum(test_func[1:])\n",
    "    Sxx = np.sum(test_func[:-1]**2)\n",
    "    Sxy = np.sum(test_func[:-1]*test_func[1:])\n",
    "    Syy = np.sum(test_func[1:]**2)\n",
    "\n",
    "    if LS:\n",
    "        a  = (n*Sxy - Sx*Sy) / (n*Sxx -Sx**2)\n",
    "        b  = (Sy - a*Sx)/n\n",
    "        sd = np.sqrt((n*Syy - Sy**2 - a*(n*Sxy - Sx*Sy))/n/(n-2))\n",
    "\n",
    "        lbda = -np.log(a)/delta\n",
    "        mu     = b/(1-a)\n",
    "        sigma  =  sd * np.sqrt( -2*np.log(a)/delta/(1-a**2))\n",
    "    else:\n",
    "        mu  = (Sy*Sxx - Sx*Sxy) / (n*(Sxx - Sxy) - (Sx**2 - Sx*Sy))\n",
    "        lbda = -np.log( (Sxy - mu*Sx - mu*Sy + n*mu**2) / (Sxx -2*mu*Sx + n*mu**2) ) / delta\n",
    "        a = np.exp(-lbda*delta)\n",
    "        sigmah2 = (Syy - 2*a*Sxy + a**2*Sxx - 2*mu*(1-a)*(Sy - a*Sx) + n*mu**2*(1-a)**2)/n\n",
    "        sigma = np.sqrt(sigmah2*2*lbda/(1-a**2))\n",
    "        \n",
    "    return mu,sigma,lbda\n",
    "#mu, sigma, lbda = ou_cal(y_ou_sin[:pred_day],1.0,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_sim(opt_data,opt_funcdata,opt_augdata, days, obs_year, obs_day, pred_day, ypred, ypred_func, ypred_aug,\n",
    "           var, var_func, var_aug, dataset, add_seas, laplace):\n",
    "    #set params\n",
    "    #add_seas = True\n",
    "    #laplace = True\n",
    "    #start point\n",
    "    t_0 = 0\n",
    "    #end point\n",
    "    t_end = days*(obs_year)+obs_day+50\n",
    "    length = t_end\n",
    "    #observ point\n",
    "    observ_point = pred_day\n",
    "    #price series\n",
    "    price_series = opt_data.ytest\n",
    "    func_price_series = opt_funcdata.ytest\n",
    "    aug_price_series = opt_augdata.ytest\n",
    "\n",
    "    #prediction\n",
    "    onedim_pred = ypred.flatten()\n",
    "    func_pred = ypred_func.flatten()\n",
    "    aug_pred = ypred_aug.flatten()\n",
    "    \n",
    "    # error bars\n",
    "    onedim_sd = np.sqrt(var).flatten()\n",
    "    funcdim_sd = np.sqrt(var_func).flatten()\n",
    "    augdim_sd = np.sqrt(var_aug).flatten()\n",
    "    \n",
    "    #underlying stoch process params\n",
    "    mu = dataset.mu\n",
    "    theta = dataset.theta\n",
    "    sigma = dataset.sigma\n",
    "\n",
    "    #########################\n",
    "    t = np.linspace(t_0,t_end,length)\n",
    "    dt = np.mean(np.diff(t))\n",
    "    drift = lambda y,t: theta*(mu-y)      # define drift term\n",
    "    diffusion = lambda y,t: sigma# define diffusion term\n",
    "\n",
    "    #noise process\n",
    "    noise = np.random.normal(loc=0.0,scale=1.0,size=length)*np.sqrt(dt) #define noise process\n",
    "\n",
    "    # solve SDE\n",
    "    #Ahrash suggestion to have the same starting point for prediction\n",
    "\n",
    "    mu_cal, sigma_cal, theta_cal = ou_cal(opt_data.y,1.0,False)    \n",
    "\n",
    "    t_0 = pred_day\n",
    "    length = t_end-t_0\n",
    "\n",
    "    num_sims = 1000\n",
    "    y_all = np.zeros([length,num_sims])\n",
    "    y_all_cal = np.zeros([length,num_sims])\n",
    "    y_all_func = np.zeros([length,num_sims])\n",
    "    y_all_cal_func = np.zeros([length,num_sims])\n",
    "    y_all_aug = np.zeros([length,num_sims])\n",
    "    y_all_cal_aug = np.zeros([length,num_sims])\n",
    "    \n",
    "    for j in range(num_sims):\n",
    "\n",
    "        drift = lambda y,t1: theta*(mu-y)      # define drift term\n",
    "        diffusion = lambda y,t1: sigma# define diffusion term\n",
    "        noise = np.random.normal(loc=0.0,scale=1.0,size=length)*np.sqrt(dt) #define noise process\n",
    "        noise1 = np.random.normal(loc=0.0,scale=1.0,size=length)*np.sqrt(dt)\n",
    "\n",
    "        #make the noise mre difficult laplace - real returns are fat tailed\n",
    "        if laplace:\n",
    "            noise = np.random.laplace(loc=0.0,scale=1.0,size=length)*np.sqrt(dt)\n",
    "            noise1 = np.random.laplace(loc=0.0,scale=1.0,size=length)*np.sqrt(dt)\n",
    "\n",
    "        y_new = np.zeros(length)\n",
    "        y_new_cal = np.zeros(length)\n",
    "\n",
    "        y_new_func = np.zeros(length)\n",
    "        y_new_cal_func = np.zeros(length)\n",
    "\n",
    "        y_new_aug = np.zeros(length)\n",
    "        y_new_cal_aug = np.zeros(length)\n",
    "    \n",
    "        #note if we are adding a function we need to take that off else the OU will mean revert too much\n",
    "        function_adjustment = dataset.sim_adjustment_val\n",
    "        \n",
    "        y_new[0] = onedim_pred[pred_day] -function_adjustment # from ahrash note\n",
    "        y_new_cal[0] = onedim_pred[pred_day] -function_adjustment\n",
    "\n",
    "        y_new_func[0] = func_pred[pred_day] -function_adjustment # from ahrash note\n",
    "        y_new_cal_func[0] = func_pred[pred_day] -function_adjustment\n",
    "\n",
    "        y_new_aug[0] = aug_pred[pred_day] -function_adjustment # from ahrash note\n",
    "        y_new_cal_aug[0] = aug_pred[pred_day] -function_adjustment\n",
    "\n",
    "        t1 = np.linspace(t_0,t_end,length)  # define time axis\n",
    "        dt = np.mean(np.diff(t1))\n",
    "\n",
    "        # solve SDE note i am adding laplace noise to the cal one - which is helping it too much\n",
    "        # although this could be an interesting aside\n",
    "\n",
    "        for i in range(1,length):\n",
    "\n",
    "            y_new[i] = np.exp(-theta*dt)*y_new[i-1] + (1- np.exp(-theta*dt))*mu  \\\n",
    "                                              + sigma*noise[i]* np.sqrt((1-np.exp(-2*theta*dt)/2*theta))\n",
    "\n",
    "            y_new_cal[i] = np.exp(-theta_cal*dt)*y_new_cal[i-1] + (1- np.exp(-theta_cal*dt))*mu_cal  \\\n",
    "                                              + sigma_cal*noise1[i]* np.sqrt((1-np.exp(-2*theta_cal*dt)/2*theta_cal))\n",
    "\n",
    "            y_new_func[i] = np.exp(-theta*dt)*y_new_func[i-1] + (1- np.exp(-theta*dt))*mu  \\\n",
    "                                              + sigma*noise[i]* np.sqrt((1-np.exp(-2*theta*dt)/2*theta))\n",
    "\n",
    "            y_new_cal_func[i] = np.exp(-theta_cal*dt)*y_new_cal_func[i-1] + (1- np.exp(-theta_cal*dt))*mu_cal  \\\n",
    "                                              + sigma_cal*noise1[i]* np.sqrt((1-np.exp(-2*theta_cal*dt)/2*theta_cal))\n",
    "\n",
    "            y_new_aug[i] = np.exp(-theta*dt)*y_new_aug[i-1] + (1- np.exp(-theta*dt))*mu  \\\n",
    "                                              + sigma*noise[i]* np.sqrt((1-np.exp(-2*theta*dt)/2*theta))\n",
    "\n",
    "            y_new_cal_aug[i] = np.exp(-theta_cal*dt)*y_new_cal_aug[i-1] + (1- np.exp(-theta_cal*dt))*mu_cal  \\\n",
    "                                              + sigma_cal*noise1[i]* np.sqrt((1-np.exp(-2*theta_cal*dt)/2*theta_cal))\n",
    "            \n",
    "        y_all[:,j] = y_new \n",
    "        y_all_cal[:,j] = y_new_cal\n",
    "\n",
    "        y_all_func[:,j] = y_new_func\n",
    "        y_all_cal_func[:,j] = y_new_cal_func\n",
    "\n",
    "        y_all_aug[:,j] = y_new_aug\n",
    "        y_all_cal_aug[:,j] = y_new_cal_aug\n",
    "\n",
    "    #now with seasonal adjustments added in   \n",
    "    #when we also have a function as well as GP we need to adjust this too\n",
    "    #add on the OU function adjustment to match with above     \n",
    "\n",
    "    seasonal = dataset.seasonal \n",
    "    ymean = y_all.mean(axis=1)\n",
    "    ystd = y_all.std(axis=1)\n",
    "    if add_seas:\n",
    "        ymean = ymean + seasonal[t_0:t_end]#-seasonal[t_0]\n",
    "\n",
    "    ymean_func = y_all_func.mean(axis=1)\n",
    "    ystd_func = y_all_func.std(axis=1)\n",
    "    if add_seas:\n",
    "        ymean_func = ymean_func + seasonal[t_0:t_end]#-seasonal[t_0]\n",
    "\n",
    "    ymean_aug = y_all_aug.mean(axis=1)\n",
    "    ystd_aug = y_all_aug.std(axis=1)\n",
    "    if add_seas:\n",
    "        ymean_aug = ymean_aug + seasonal[t_0:t_end]#-seasonal[t_0]\n",
    "\n",
    "    ymean_cal = y_all_cal.mean(axis=1)\n",
    "    ystd_cal = y_all_cal.std(axis=1)\n",
    "    ymean_cal = ymean_cal #note the AR(1) doesnt know about the function so adjustment the\n",
    "    #the other way here!\n",
    "    if add_seas:\n",
    "        ymean_cal = ymean_cal +function_adjustment\n",
    "\n",
    "    ymean_cal_func = y_all_cal_func.mean(axis=1)\n",
    "    ystd_cal_func = y_all_cal_func.std(axis=1)\n",
    "    ymean_cal_func = ymean_cal_func \n",
    "\n",
    "    ymean_cal_aug = y_all_cal_aug.mean(axis=1)\n",
    "    ystd_cal_aug = y_all_cal_aug.std(axis=1)\n",
    "    ymean_cal_aug = ymean_cal_aug \n",
    "    \n",
    "    ret_vars = (t1,ymean, ystd, ymean_cal, ystd_cal, onedim_pred, t_0, t_end, onedim_sd, func_pred, ymean_func,\n",
    "               ystd_func, funcdim_sd, aug_pred, ymean_aug, ystd_aug, augdim_sd,mu_cal, sigma_cal, theta_cal)\n",
    "    return ret_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mk_tst_metric(ymean,ymean_cal,onedim_pred, func_pred, aug_pred, ystd, ystd_cal, onedim_sd, ystd_func, funcdim_sd\n",
    "                 ,ystd_aug, augdim_sd):\n",
    "    \n",
    "    mse_cal = np.zeros(ymean.shape[0])\n",
    "    mse_1d = np.zeros(ymean.shape[0])\n",
    "    mse_func = np.zeros(ymean.shape[0])\n",
    "    mse_aug = np.zeros(ymean.shape[0])\n",
    "\n",
    "    for i in range(ymean.shape[0]):\n",
    "        mse_cal[i] = np.cumsum((ymean-ymean_cal)**2)[i]/(i+1)\n",
    "        mse_1d[i] = np.cumsum((ymean-onedim_pred[t_0:t_end])**2)[i]/(i+1)\n",
    "        mse_func[i] = np.cumsum((ymean_func-func_pred[t_0:t_end])**2)[i]/(i+1)\n",
    "        mse_aug[i] = np.cumsum((ymean_aug-aug_pred[t_0:t_end])**2)[i]/(i+1)\n",
    "\n",
    "    mse_1d_std = np.zeros(ymean.shape[0])\n",
    "    mse_func_std = np.zeros(ymean.shape[0])\n",
    "    mse_aug_std = np.zeros(ymean.shape[0])\n",
    "    mse_cal_std = np.zeros(ymean.shape[0])\n",
    "    \n",
    "    for i in range(ymean.shape[0]):\n",
    "        mse_cal_std[i] = np.cumsum((ystd-ystd_cal)**2)[i]/(i+1)\n",
    "        mse_1d_std[i] = np.cumsum((ystd-onedim_sd[t_0:t_end])**2)[i]/(i+1)\n",
    "        mse_func_std[i] = np.cumsum((ystd_func-funcdim_sd[t_0:t_end])**2)[i]/(i+1)\n",
    "        mse_aug_std[i] = np.cumsum((ystd_aug-augdim_sd[t_0:t_end])**2)[i]/(i+1)\n",
    "        \n",
    "    ret_vars = mse_cal, mse_1d, mse_func, mse_aug, mse_1d_std, mse_func_std, mse_aug_std, mse_cal_std\n",
    "    return ret_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_preds(ymean,ystd,t1,ymean_cal,ystd_cal,onedim_pred, onedim_sd, func_pred, ymean_func, ystd_func,\n",
    "              funcdim_sd, aug_pred, ystd_aug, ymean_aug, augdim_sd):\n",
    "\n",
    "    #f, axarr = plt.subplots(2, 2)\n",
    "    f, axarr= plt.subplots(2,2,figsize=(20, 10))\n",
    "\n",
    "    axarr[0, 0].fill_between(t1, (ymean- ystd), \n",
    "                     (ymean+ ystd), color='blue', alpha=.3, label='SD')\n",
    "    axarr[0, 0].plot(t1,ymean_cal, label='cal')\n",
    "    axarr[0, 0].fill_between(t1, (ymean_cal- ystd_cal), \n",
    "                     (ymean_cal+ ystd_cal), color='yellow', alpha=.3, label='Cal SD')\n",
    "    axarr[0, 0].plot(t1,ymean, label='mean')\n",
    "    axarr[0, 0].set_title('AR(1)') \n",
    "\n",
    "    \n",
    "    axarr[0, 1].plot(t1,onedim_pred[t_0:t_end], label='1d_pred')\n",
    "    axarr[0, 1].fill_between(t1, (ymean- ystd), \n",
    "                     (ymean+ ystd), color='blue', alpha=.3, label='SD')\n",
    "    axarr[0, 1].plot(t1,ymean, label='mean')\n",
    "    axarr[0, 1].fill_between(t1, onedim_pred[t_0:t_end]- onedim_sd[t_0:t_end], \n",
    "            (onedim_pred[t_0:t_end] + onedim_sd[t_0:t_end]), color='yellow', alpha=.5, label='Conf Int')\n",
    "    axarr[0, 1].set_title('1 dim GP')\n",
    "    #axarr[0, 1].legend() \n",
    "\n",
    "    axarr[1, 0].plot(t1,func_pred[t_0:t_end], label='func_pred')\n",
    "    axarr[1, 0].fill_between(t1, (ymean_func- ystd_func), \n",
    "                     (ymean_func+ ystd_func), color='blue', alpha=.3, label='SD')\n",
    "    axarr[1, 0].plot(t1,ymean_func, label='mean')\n",
    "    axarr[1, 0].fill_between(t1, func_pred[t_0:t_end]- funcdim_sd[t_0:t_end], \n",
    "            (func_pred[t_0:t_end] + funcdim_sd[t_0:t_end]), color='yellow', alpha=.5, label='Conf Int')\n",
    "    #axarr[1, 0].legend() \n",
    "    axarr[1,0].set_title('Func GP')\n",
    "\n",
    "    axarr[1, 1].plot(t1,aug_pred[t_0:t_end], label='aug_pred')\n",
    "    axarr[1, 1].fill_between(t1, (ymean_aug- ystd_aug), \n",
    "                     (ymean_aug+ ystd_aug), color='blue', alpha=.3, label='SD')\n",
    "    axarr[1, 1].plot(t1,ymean_aug, label='mean')\n",
    "    axarr[1, 1].fill_between(t1, aug_pred[t_0:t_end]- augdim_sd[t_0:t_end], \n",
    "            (aug_pred[t_0:t_end] + augdim_sd[t_0:t_end]), color='yellow', alpha=.5, label='Conf Int')\n",
    "    axarr[1,1].set_title('Aug GP')\n",
    "\n",
    "    \n",
    "    f.subplots_adjust(hspace=0.3)\n",
    "    f.subplots_adjust(wspace=0.3)\n",
    "\n",
    "    #axarr[1, 1].legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_mse(mse_cal, mse_1d, mse_func, mse_aug):\n",
    "\n",
    "    f, axarr = plt.subplots(1,2, sharex=True, figsize=(20, 10))\n",
    "\n",
    "    axarr[0].plot(mse_cal, label='cal')\n",
    "    axarr[0].plot(mse_1d, label='1d')\n",
    "    axarr[0].plot(mse_func, label='func')\n",
    "    axarr[0].plot(mse_aug, label='aug')\n",
    "\n",
    "    axarr[0].legend()\n",
    "    axarr[1].plot(mse_cal_std, label='cal')\n",
    "    axarr[1].plot(mse_1d_std, label='1d')\n",
    "    axarr[1].plot(mse_func_std, label='func')\n",
    "    axarr[1].plot(mse_aug_std, label='aug')\n",
    "    axarr[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class settings(object):\n",
    "    def __init__(self, years, days, obs_year, obs_day, reset2zero, max_delta, samples, add_seas, laplace, sigma, kern):\n",
    "        self.years = years\n",
    "        self.days = days\n",
    "        self.obs_year = obs_year\n",
    "        self.obs_day = obs_day\n",
    "        self.reset2zero = reset2zero\n",
    "        self.pred_day = days*obs_year+obs_day\n",
    "        self.max_delta = max_delta\n",
    "        self.samples = samples\n",
    "        self.add_seas = add_seas\n",
    "        self.laplace = laplace\n",
    "        self.sigma = sigma\n",
    "        self.kern = kern\n",
    "        \n",
    "        self.aug1dyears = 1\n",
    "        self.aug1ddays = years*days\n",
    "        self.aug1dobs_year = 0\n",
    "        self.aug1dobs_day = days*obs_year+obs_day\n",
    "        self.aug1dpred_day = days*obs_year+obs_day\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "add_seas = True\n",
    "laplace = False\n",
    "sigma = 0.1 #OU sigma - rest is mean zero, \n",
    "kern = \"rat\"\n",
    "#Set what data we are using set train/ test periods\n",
    "\n",
    "years = 10\n",
    "days = 250\n",
    "obs_year = 5#0#4#3 #test1\n",
    "obs_day = 20#800#50 #95\n",
    "reset2zero =False\n",
    "#pred_day = days*obs_year+obs_day\n",
    "\n",
    "#for augmented data set our max look forward and samples - it also has this year's delta zeros\n",
    "max_delta = 50\n",
    "samples = 500\n",
    "\n",
    "s = settings(years,days,obs_year,obs_day,reset2zero,max_delta,samples, add_seas, laplace, sigma, kern)\n",
    "\n",
    "#optimisation runs\n",
    "restarts =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Users/johngoodacre/anaconda/envs/gp/lib/python3.6/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning:\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Create raw datasets\n",
    "\n",
    "#1d or functional\n",
    "dataset = data(seed, \"ou\",s.years,s.days,s.obs_year,s.obs_day, s.reset2zero, s.add_seas, s.laplace, s.sigma)\n",
    "dataset.mk_tr_tst()\n",
    "all_train_data, all_test_data = dataset.all_train, dataset.all_test\n",
    "\n",
    "#augmented\n",
    "dataset.get_aug_raw_data()\n",
    "dataset.mk_aug_tr_tst(s.max_delta)\n",
    "dataset.sample(s.samples)\n",
    "\n",
    "samp_train_data, samp_test_data = dataset.samp_aug_train, dataset.samp_aug_test\n",
    "\n",
    "#augmented 1d\n",
    "dataset1d = data(seed, \"ou\",s.aug1dyears,s.aug1ddays,s.aug1dobs_year,s.aug1dobs_day, s.reset2zero\n",
    "                 ,s.add_seas, s.laplace, s.sigma)\n",
    "dataset1d.get_aug_raw_data()\n",
    "dataset1d.mk_aug_tr_tst(s.max_delta)\n",
    "dataset1d.sample(s.samples)\n",
    "\n",
    "samp_train_data1d, samp_test_data1d = dataset1d.samp_aug_train, dataset1d.samp_aug_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset.plot()\n",
    "#dataset.plot_auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running L-BFGS-B (Scipy implementation) Code:\n",
      "  runtime   i      f              |g|        \n",
      "    03s54  0007   3.498595e+05   1.061813e+11 \n",
      "    08s76  0017  -9.047836e+02   3.780825e+02 \n",
      "    14s82  0029  -9.147857e+02   2.192395e+01 \n",
      "    22s73  0044  -9.161648e+02   4.865254e-01 \n",
      "    29s13  0056  -9.161930e+02   4.598925e-07 \n",
      "    29s73  0057  -9.161930e+02   4.598925e-07 \n",
      "Runtime:     29s73\n",
      "Optimization status: Converged\n",
      "\n",
      "Optimization restart 1/1, f = -916.1930449807369\n",
      "Running L-BFGS-B (Scipy implementation) Code:\n",
      "  runtime   i      f              |g|        \n",
      "    01s18  0002   1.988838e+03   2.211788e+05 \n",
      "    06s36  0012  -9.427588e+02   1.116912e+02 \n",
      "    12s32  0024  -9.588009e+02   2.711394e+01 \n",
      "    24s20  0047  -9.604895e+02   6.044376e-01 \n",
      "    26s44  0051  -9.604906e+02   2.375465e-04 \n",
      "    27s38  0053  -9.604906e+02   2.484954e-04 \n",
      "Runtime:     27s38\n",
      "Optimization status: Converged\n",
      "\n",
      "Optimization restart 1/1, f = -960.4905895486545\n",
      "Running L-BFGS-B (Scipy implementation) Code:\n",
      "  runtime   i      f              |g|        \n",
      "    00s05  0001   1.777453e+03   1.552316e+05 \n",
      "    00s16  0003   1.307804e+03   3.704094e+04 \n",
      "    01s20  0024   1.568037e+02   1.372404e+03 \n",
      "    02s21  0045  -3.751223e+00   6.295776e+00 \n",
      "    04s27  0085   4.674416e+03   1.846121e+07 \n",
      "    07s34  0147  -1.227770e+01   2.400859e-01 \n",
      "    09s48  0188  -1.230720e+01   4.351403e-04 \n",
      "    10s53  0206  -1.230763e+01   4.498238e-06 \n",
      "    11s50  0225  -1.230771e+01   5.345542e-07 \n",
      "Runtime:     11s50\n",
      "Optimization status: Converged\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Users/johngoodacre/anaconda/envs/gp/lib/python3.6/site-packages/paramz-0.7.3-py3.6.egg/paramz/transformations.py:109: RuntimeWarning:overflow encountered in expm1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 1/1, f = -12.307705588962676\n",
      "Running L-BFGS-B (Scipy implementation) Code:\n",
      "  runtime   i      f              |g|        \n",
      "    00s00  0000   5.149627e+03           nan \n",
      "    09s25  0009  -2.600591e+02   2.958738e+05 \n",
      "    18s58  0018  -8.949662e+02   2.315427e+03 \n",
      "    33s49  0031  -9.462222e+02   6.813503e+00 \n",
      "    58s17  0056  -9.659079e+02   2.961778e-01 \n",
      " 01m01s46  0060  -9.659078e+02   7.949359e-01 \n",
      " 01m03s17  0062  -9.659096e+02   7.394329e-05 \n",
      "Runtime:  01m03s17\n",
      "Optimization status: Converged\n",
      "\n",
      "Optimization restart 1/1, f = -965.9096301965885\n"
     ]
    }
   ],
   "source": [
    "#Optimisations\n",
    "#Basic 1-d GP\n",
    "feat_lst = ['Day_Index']\n",
    "opt_data=opt(all_train_data,all_test_data,feat_lst, s.years, s.days, s.obs_year, s.obs_day)\n",
    "\n",
    "ypred, var, ll = opt_data.run_opt(s.kern,restarts)\n",
    "\n",
    "#functional \n",
    "feat_lst = ['Year', 'Day'] \n",
    "opt_funcdata=opt(all_train_data,all_test_data,feat_lst, s.years, s.days, s.obs_year, s.obs_day)\n",
    "\n",
    "ypred_func, var_func, ll_func = opt_funcdata.run_opt(s.kern,restarts) \n",
    "\n",
    "#Augmented\n",
    "feat_lst = ['Year', 'Ob_day','Delta','OPrice','S1','S2','S3'] \n",
    "opt_augdata=opt(samp_train_data,samp_test_data,feat_lst, s.years, s.days, s.obs_year, s.obs_day)\n",
    "\n",
    "ypred_aug, var_aug, ll_aug = opt_augdata.run_opt(s.kern,restarts, True)\n",
    "\n",
    "#Augmented 1d\n",
    "feat_lst = ['Ob_day','Delta','OPrice','S1','S2','S3'] \n",
    "opt_1daugdata=opt(samp_train_data1d,samp_test_data1d,feat_lst, s.aug1dyears, s.aug1ddays, \n",
    "                  s.aug1dobs_year, s.aug1dobs_day)\n",
    "\n",
    "ypred_1daug, var_1daug, ll_1daug = opt_1daugdata.run_opt(s.kern,restarts, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Produce charts\n",
    "\n",
    "title = \"GP regression optimised\"\n",
    "filename = \"opt.png\"\n",
    "first_day = s.days*s.obs_year+1\n",
    "num_days = s.days*(s.obs_year+1)\n",
    "obs_day = s.days*s.obs_year+s.obs_day\n",
    "\n",
    "xfit = np.linspace(0, len(ypred), len(ypred)).reshape(len(ypred), 1).flatten()\n",
    "ytest = opt_data.ytest\n",
    "onedargs = (title, filename, obs_day, first_day, num_days, xfit, ypred, ytest, var, ll)\n",
    "\n",
    "title = \"func GP regression optimised\"\n",
    "filename = \"funcopt.png\"\n",
    "ytest_func = opt_funcdata.ytest\n",
    "funcargs = (title, filename, obs_day, first_day, num_days, xfit, ypred_func, ytest_func, var_func, ll_func)\n",
    "\n",
    "title = \"Aug GP regression optimised\"\n",
    "filename = \"augopt.png\"\n",
    "ytest_aug = opt_augdata.ytest\n",
    "augargs = (title, filename, obs_day, first_day, num_days, xfit, ypred_aug, ytest_aug, var_aug, ll_aug)\n",
    "\n",
    "\n",
    "title = \"Aug GP 1d regression optimised\"\n",
    "filename = \"aug1dopt.png\"\n",
    "ytest_aug1d = opt_1daugdata.ytest\n",
    "aug1dargs = (title, filename, s.aug1dobs_day, first_day, num_days, xfit, ypred_1daug, ytest_aug1d, var_1daug, ll_1daug)\n",
    "\n",
    "all_args = (onedargs,funcargs,augargs,aug1dargs)\n",
    "#plot_result1(all_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1, ymean, ystd, ymean_cal, ystd_cal, onedim_pred, t_0, t_end, onedim_sd, func_pred, ymean_func, ystd_func,\\\n",
    "    funcdim_sd, aug_pred, ymean_aug, ystd_aug, augdim_sd, mu_cal, sigma_cal, theta_cal \\\n",
    "                = run_sim(opt_data,opt_funcdata,opt_augdata, s.days, s.obs_year, s.obs_day, s.pred_day, \n",
    "                          ypred, ypred_func, ypred_aug, var, var_func, var_aug, dataset, add_seas,laplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_cal, mse_1d, mse_func, mse_aug, mse_1d_std, mse_func_std, mse_aug_std, mse_cal_std \\\n",
    "    =mk_tst_metric(ymean,ymean_cal,onedim_pred, func_pred, aug_pred, ystd, ystd_cal, onedim_sd, ystd_func, funcdim_sd\n",
    "                 ,ystd_aug, augdim_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show_preds(ymean,ystd,t1,ymean_cal,ystd_cal,onedim_pred, onedim_sd, func_pred, ymean_func, ystd_func,\n",
    "#              funcdim_sd, aug_pred,ystd_aug, ymean_aug, augdim_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show_mse(mse_cal, mse_1d, mse_func, mse_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(mu_cal)\n",
    "#print(sigma_cal)\n",
    "#print(theta_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['Seed','Pred_day','Max_Delta','Samples', 'Kernel', 'Noise', 'Laplace','Pred_level',\n",
    "           'AR1_MSE_10','AR1_MSE_20','AR1_MSE_30', 'AR1_MSESD_10', 'AR1_MSESD_20', 'AR1_MSESD_30',\n",
    "          '1D_MSE_10','1D_MSE_20','1D_MSE_30','1D_MSESD_10','1D_MSESD_20','1D_MSESD_30',\n",
    "          'FUNC_MSE_10','FUNC_MSE_20','FUNC_MSE_30','FUNC_MSESD_10','FUNC_MSESD_20','FUNC_MSESD_30',\n",
    "          'AUG_MSE_10','AUG_MSE_20','AUG_MSE_30','AUG_MSESD_10','AUG_MSESD_20','AUG_MSESD_30'] #,\n",
    "          #'AUD1D_MSE_10','AUG1D_MSE_20','AUG1D_MSE_30','AUG1D_MSESD_10','AUG1D_MSE_20','AUG1D_MSE_30']\n",
    "results = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_results = [seed,s.pred_day,max_delta, s.samples, s.kern,s.sigma,s.laplace,ymean[0]]\n",
    "ar1 = [mse_cal[10],mse_cal[20],mse_cal[30],mse_cal_std[10],mse_cal_std[20],mse_cal_std[30]]\n",
    "oned = [mse_1d[10],mse_1d[20],mse_1d[30],mse_1d_std[10],mse_1d_std[20],mse_1d_std[30]]\n",
    "func = [mse_func[10],mse_func[20],mse_func[30],mse_func_std[10],mse_func_std[20],mse_func_std[30]]\n",
    "aug = [mse_aug[10],mse_aug[20],mse_aug[30],mse_aug_std[10],mse_aug_std[20],mse_aug_std[30]]\n",
    "\n",
    "run_results = run_results+ ar1+oned+func+aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = pd.Series(run_results, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = results.append([row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seed</th>\n",
       "      <th>Pred_day</th>\n",
       "      <th>Max_Delta</th>\n",
       "      <th>Samples</th>\n",
       "      <th>Kernel</th>\n",
       "      <th>Noise</th>\n",
       "      <th>Laplace</th>\n",
       "      <th>Pred_level</th>\n",
       "      <th>AR1_MSE_10</th>\n",
       "      <th>AR1_MSE_20</th>\n",
       "      <th>...</th>\n",
       "      <th>FUNC_MSE_30</th>\n",
       "      <th>FUNC_MSESD_10</th>\n",
       "      <th>FUNC_MSESD_20</th>\n",
       "      <th>FUNC_MSESD_30</th>\n",
       "      <th>AUG_MSE_10</th>\n",
       "      <th>AUG_MSE_20</th>\n",
       "      <th>AUG_MSE_30</th>\n",
       "      <th>AUG_MSESD_10</th>\n",
       "      <th>AUG_MSESD_20</th>\n",
       "      <th>AUG_MSESD_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1270</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>rat</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>2.932185</td>\n",
       "      <td>0.186888</td>\n",
       "      <td>0.607887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288285</td>\n",
       "      <td>0.042395</td>\n",
       "      <td>0.073805</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.009217</td>\n",
       "      <td>0.031246</td>\n",
       "      <td>0.043085</td>\n",
       "      <td>0.003065</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>0.004246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Seed Pred_day Max_Delta Samples Kernel  Noise Laplace  Pred_level  \\\n",
       "0    0     1270        50     500    rat    0.1   False    2.932185   \n",
       "\n",
       "   AR1_MSE_10  AR1_MSE_20      ...       FUNC_MSE_30  FUNC_MSESD_10  \\\n",
       "0    0.186888    0.607887      ...          0.288285       0.042395   \n",
       "\n",
       "   FUNC_MSESD_20  FUNC_MSESD_30  AUG_MSE_10  AUG_MSE_20  AUG_MSE_30  \\\n",
       "0       0.073805       0.081213    0.009217    0.031246    0.043085   \n",
       "\n",
       "   AUG_MSESD_10  AUG_MSESD_20  AUG_MSESD_30  \n",
       "0      0.003065      0.002007      0.004246  \n",
       "\n",
       "[1 rows x 32 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {
    "189b828112204974ac5b21449e422f94": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    },
    "21960c798fe7452cb6b45124a5333864": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "2b022556256548ba9523b366221b2367": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "2d6b535a7a004b11991d29eaad74bd4d": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "44beb57a4d044484871fd53cdf9a3046": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "46ab15dbee5e4858a7357738a9285a77": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "4b95b3d732b249de8a06864d442cbed6": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "57f5a071000743218e0a2ae167e99cb2": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "8bd1901f18fe481694c91fac4f5dbd65": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "8f916a52e7964623a1079d0ecf0a077f": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "acb75e70b2424158a9ab34f041cca846": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    },
    "b20d78fdfaa64b9cb20a3ddf4dad0135": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "c37ba13b1a1b4f828fbb7f6c1fcf6a42": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "d618c2a7bbf042c19e042db29cb5ca72": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
